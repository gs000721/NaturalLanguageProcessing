{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sentence Tokenization"
      ],
      "metadata": {
        "id": "e22r-ALEKm9T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "31BcORZ3JSiD"
      },
      "outputs": [],
      "source": [
        "import spacy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "_U7XfGXmJc3B"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the paragraph to be sentence tokenized\n",
        "paragraph = \"Tokenization is the process of breaking down a text or a sentence into smaller units, known as tokens. This is useful in natural language processing, where we want to analyze the underlying meaning of a piece of text. There are several libraries available for performing sentence tokenization, including spaCy.\"\n"
      ],
      "metadata": {
        "id": "5e-V3XV5JpH8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a spaCy Doc object from the paragraph\n",
        "doc = nlp(paragraph)\n",
        "#type(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrdoYoG_Jqc7",
        "outputId": "afc59f62-e8dd-4b49-d3e4-f63dc22701b5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.doc.Doc"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the sentences from the Doc object\n",
        "sentences = list(doc.sents)"
      ],
      "metadata": {
        "id": "72aEoPpGJ6Ol"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJUJmiMgJ-52",
        "outputId": "7e43c8b0-239e-4287-d10d-8d8a58a7123c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Tokenization is the process of breaking down a text or a sentence into smaller units, known as tokens.,\n",
              " This is useful in natural language processing, where we want to analyze the underlying meaning of a piece of text.,\n",
              " There are several libraries available for performing sentence tokenization, including spaCy.]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Tokenization"
      ],
      "metadata": {
        "id": "ZWLzw4ALK0Yk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "GzgB3u5MKSk-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "GoM_QB9KK7QK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"I got the eye of the tiger, fighter, dancing through the fire and you're gonna hear me roar, louder, louder than a lion.\""
      ],
      "metadata": {
        "id": "zfiJKoAtLODo"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(paragraph)"
      ],
      "metadata": {
        "id": "cJ0iPPaDLcB2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [token.text for token in doc]"
      ],
      "metadata": {
        "id": "hoVWHyzHLgMb"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VEk0Ld3L2aA",
        "outputId": "aa43bf4c-2403-4938-cfc5-ef49baa97659"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'got',\n",
              " 'the',\n",
              " 'eye',\n",
              " 'of',\n",
              " 'the',\n",
              " 'tiger',\n",
              " ',',\n",
              " 'fighter',\n",
              " ',',\n",
              " 'dancing',\n",
              " 'through',\n",
              " 'the',\n",
              " 'fire',\n",
              " 'and',\n",
              " 'you',\n",
              " \"'re\",\n",
              " 'gon',\n",
              " 'na',\n",
              " 'hear',\n",
              " 'me',\n",
              " 'roar',\n",
              " ',',\n",
              " 'louder',\n",
              " ',',\n",
              " 'louder',\n",
              " 'than',\n",
              " 'a',\n",
              " 'lion',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RVNAe2JoL8hB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}